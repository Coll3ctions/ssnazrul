{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OAuth Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to scrape twitter data and do a tf-idf analysis on that (src-uwes twitter analysis). We will need OAuth authentication, and we will follow a similar approach as detailed in the yelp analysis notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib\n",
    "import json, operator\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to define the following variables\n",
    "#api_key = #<get api key> \n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "\n",
    "#defining them right here is not safe. insteadm create a file in a different directory\n",
    "# (I use ~/VaultDSE) and in it put a file called, say, twitterkeys.py whose\n",
    "# content is:\n",
    "#api_key = #<get api key>\n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "#\n",
    "#def getkeys():\n",
    "#    return api_key,api_secret,access_token_key,access_token_secret\n",
    "\n",
    "# then use the following commands\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/sadat/Documents/DSE/VaultDSE')\n",
    "import twitterKeys\n",
    "api_key,api_secret,access_token_key,access_token_secret=twitterKeys.getkeys()\n",
    "\n",
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "  req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "  req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "  headers = req.to_header()\n",
    "\n",
    "  if http_method == \"POST\":\n",
    "    encoded_post_data = req.to_postdata()\n",
    "  else:\n",
    "    encoded_post_data = None\n",
    "    url = req.to_url()\n",
    "\n",
    "  opener = urllib.OpenerDirector()\n",
    "  opener.add_handler(http_handler)\n",
    "  opener.add_handler(https_handler)\n",
    "\n",
    "  response = opener.open(url, encoded_post_data)\n",
    "\n",
    "  return response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchData(url):\n",
    "    response = getTwitterStream(url, \"GET\", [])\n",
    "    lines = response.read()\n",
    "    j = json.loads(lines)\n",
    "    h = j['statuses']\n",
    "    print 'Stream: ',url.split('/')[-1][14:], '\\n'\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            print i+1\n",
    "            print h[i]['text'],'\\n'\n",
    "        except:\n",
    "            continue\n",
    "    print '\\n\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream:  UCSD \n",
      "\n",
      "1\n",
      "RT @Keysight: Keysight collaborates with #UCSD to demonstrate world‚Äôs first #5G, 100-200 meter communication link up to 2 Gbps: https://t.c‚Ä¶ \n",
      "\n",
      "2\n",
      "World's First 5G, 100 To 200 Meter Comms Link Up To 2 Gbps Demo'd By Keysight Technologies + UCSD https://t.co/n3aow4n1Iq #science \n",
      "\n",
      "3\n",
      "RT @augmentl: HTC Vive Tour Hits NC State and UCSD Universities this Week https://t.co/ppEZMnwE1H #virtualreality #vr \n",
      "\n",
      "4\n",
      "RT @FamilyoftheYear: San Diego ü§ë see ya tonight @ the loft #ucsd \n",
      "\n",
      "5\n",
      "Climate models underestimate the observed deoxygenation of oxygen minimum zones - Corinne Le Quere #OceanforClimate @Scripps_Ocean #ucsd \n",
      "\n",
      "6\n",
      "RT @Keysight: Keysight collaborates with #UCSD to demonstrate world‚Äôs first #5G, 100-200 meter communication link up to 2 Gbps: https://t.c‚Ä¶ \n",
      "\n",
      "7\n",
      "With organizers and speakers of the Tara Expeditions ocean science event @Scripps_Ocean #OceanforClimate #ucsd https://t.co/fUC1aslJT1 \n",
      "\n",
      "8\n",
      "RT @ArchivosEst: They still draw pictures. Drawings made by Spanish children during the #SpanishCivilWar https://t.co/OLUJLdEVZe \n",
      "\n",
      "#Memoria‚Ä¶ \n",
      "\n",
      "9\n",
      "L. Levin at Tara #COP21: Protect oceans w/education \"we need to ensure that the next gen cares about this\" #ucsd #oceanforclimate #climateuc \n",
      "\n",
      "10\n",
      "HTC Vive Tour Hits NC State and UCSD Universities this Week https://t.co/ppEZMnwE1H #virtualreality #vr \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stream:  Donald Trump \n",
      "\n",
      "1\n",
      "RT @PoliticalLaughs: BREAKING NEWS: Donald Trump cancels meeting with Jesus after Jesus refuses to endorse him. \n",
      "\n",
      "2\n",
      "RT @healthandcents: GREAT article! üëçüòä\n",
      "‚ù§Ô∏èüá∫üá∏‚û°Ô∏è\"Admit it. Donald Trump is a volcano in forest of Ronson lighters\"\n",
      "\n",
      ".@realDonaldTrump #Trump ht‚Ä¶ \n",
      "\n",
      "3\n",
      "RT @Eating: RT if you think this mozzarella stick would be a better president than Donald Trump. https://t.co/xwXui5MeYr \n",
      "\n",
      "4\n",
      "@julesmattsson reminds me of the way Donald Trump reminds everyone how unprofitable all his critics are \n",
      "\n",
      "5\n",
      "Donald Trump Retweets Message Calling Chrissy Teigen \"Trashy Gutter Mouth Woman\" https://t.co/dPTW9YYROL \n",
      "\n",
      "6\n",
      "Kh√°t v·ªçng th√†nh c√¥ng\n",
      "      V·∫≠y l√† t√¥i ƒë√£ k·∫øt th√∫c m·ªôt ph·∫ßn c√¥ng vi·ªác t·∫ª nh·∫°t v√† t√¥i kh√¥ng th√≠ch. C√≥ l·∫Ω ,,, https://t.co/HE0EI3p2J1 \n",
      "\n",
      "7\n",
      "https://t.co/7NbBCK3XMa... https://t.co/fo2xJr6bZy \n",
      "\n",
      "8\n",
      "Donald Trump suggests CNN pay him $5 million or he'll boycott next GOP #debate https://t.co/FkNm2VLFkw https://t.co/pcWSErI2aq \n",
      "\n",
      "9\n",
      "RT @g_benderski: VOTE FOR TRUMP! \n",
      "Hey Donald! Here is your new campaign logo! Thank me later!\n",
      "#DonaldTrump ‚Ä¶ https://t.co/XFHP3B6XQZ https:‚Ä¶ \n",
      "\n",
      "10\n",
      "#SuddenRealizations Duterte is the Donald Trump of the Philippines O_O except Duterte's a little bit more rational \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stream:  Syria \n",
      "\n",
      "1\n",
      "#syria ÿßŸÑÿ®ŸÜÿ™ÿßÿ∫ŸàŸÜ ŸäŸÜÿßÿ¥ÿØ ÿ≥ŸÑÿßÿ≠ ÿßŸÑÿ¨Ÿà ÿßŸÑÿ±Ÿàÿ≥Ÿä ŸÅŸä ÿ≥Ÿàÿ±Ÿäÿß ÿπÿØŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿµŸàÿßÿ±ŸäÿÆ \"ÿ¨Ÿà-ÿ¨Ÿà\" ÿ∂ÿØ ÿ∑ÿßÿ¶ÿ±ÿßÿ™ ÿßŸÑÿ™ÿ≠ÿßŸÑŸÅ - ÿ±Ÿàÿ≥Ÿäÿß ÿßŸÑŸäŸàŸÖ https://t.co/vKFyu6swAN \n",
      "\n",
      "2\n",
      "RT @STWuk: Maximise pressure on MPs. Follow #DontBombSyria Action Page for updates on protests https://t.co/rGOSaBf7WE https://t.co/uJiLZml‚Ä¶ \n",
      "\n",
      "3\n",
      "RT @Conflicts: BREAKING: German Cabinet approves mission in #Syria against 'Islamic State' https://t.co/2xSSuCUxML - @dwnews \n",
      "\n",
      "4\n",
      "RT @STWuk: Maximise pressure on MPs. Follow #DontBombSyria Action Page for updates on protests https://t.co/rGOSaBf7WE https://t.co/uJiLZml‚Ä¶ \n",
      "\n",
      "5\n",
      "VIDEO: PM seeks cabinet support over Syria https://t.co/mmmKeO3BKc \n",
      "\n",
      "6\n",
      "#syria ÿßÿÆÿ± ÿßÿÆÿ®ÿßÿ± ÿ≥Ÿàÿ±Ÿäÿß ÿßŸÑÿßŸÜ ÿßŸÑÿ£ÿ≥ÿØ ŸÑÿ™ŸÑŸÅÿ≤ŸäŸàŸÜ ÿßŸÑÿ™ÿ¥ŸäŸÉ: ŸäŸàÿ¨ÿØ ÿ•ÿ±Ÿáÿßÿ®ŸäŸàŸÜ ÿ®ŸäŸÜ ŸÑŸÑÿßÿ¨ÿ¶ŸäŸÜ ÿßŸÑÿ≥Ÿàÿ±ŸäŸäŸÜ - ÿ®Ÿàÿßÿ®ÿ™ŸÉ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© https://t.co/wqjsxcg8Pv \n",
      "\n",
      "7\n",
      "Who agrees with Ken? :S https://t.co/yAdYKUhDbZ \n",
      "\n",
      "8\n",
      "Who agrees with Ken? :S https://t.co/cQeWBKJNOt \n",
      "\n",
      "9\n",
      "RT @Brown_Saraah: There are rivers of blood in Syria but that isn't enough for mainstream media to report what's happening there. http://t.‚Ä¶ \n",
      "\n",
      "10\n",
      "Who agrees with Ken? :S https://t.co/uRA1LpQavO \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_query = ['UCSD', 'Donald Trump', 'Syria']\n",
    "\n",
    "for search_query in list_query:\n",
    "    #We can also request twitter stream data for specific search parameters as follows\n",
    "    url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "    fetchData(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf‚Äìidf, short for term frequency‚Äìinverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Multiply the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Stream file generated\n"
     ]
    }
   ],
   "source": [
    "#1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "writer = open('twitterStream.txt', 'a') \n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+\"syria\"\n",
    "response = getTwitterStream(url, \"GET\", [])\n",
    "lines = response.read()\n",
    "j = json.loads(lines)\n",
    "h = j['statuses']\n",
    "for i in range(200):\n",
    "    try:\n",
    "        writer.write(h[i]['text']+'\\n')\n",
    "    except:\n",
    "        continue\n",
    "writer.close()\n",
    "\n",
    "print 'Twitter Stream file generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency:\n",
      "\n",
      "\n",
      "{'and': 1, 'cons': 1, 'https://t.co/mmmkeo3bkc': 1, 'german': 1, 'pros': 1, 'over': 1, 'https://t.co/cqewbkjnot': 1, 'mission': 1, 'wednesday': 1, 'vote': 1, 'us': 1, 'in': 2, 'go': 1, 'says': 1, 'breaking:': 1, \"'islamic\": 1, 'agrees': 3, 'https://t.co/2xssucuxml': 1, 'rt': 2, 'guess': 1, 'https://t.co/ura1lpqavo': 1, 'sadly': 1, 'i': 1, 'no': 1, \"it's\": 1, 'support': 1, '-': 1, 'should': 1, 'better': 1, 'to': 2, 'out': 1, 'war': 1, 'seeks': 1, 'pm': 1, 'ken': 3, '@conflicts:': 1, \"state'\": 1, 'on': 1, '@dwnews': 1, 'approves': 1, 'who': 3, 'but': 1, 'video:': 1, 'hear': 1, 'opinion': 1, 'not': 1, 'cabinet': 2, 'with': 3, 'is': 1, ':s': 3, 'well': 1, '#syria': 1, 'being': 1, 'we': 1, 'https://t.co/yadykuhdbz': 1, '@skynews:': 1, 'against': 1, 'public': 1, 'this': 1, 'bbc': 1, 'https://t.co/zv8kovbuwo': 1, 'time': 1, 'syria': 3, 'the': 1, 'asked': 1}\n"
     ]
    }
   ],
   "source": [
    "#2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "\n",
    "def tf(name):\n",
    "    '''Term Frequency'''\n",
    "    char = '.,?\"'\n",
    "    text = open(name, 'r')\n",
    "    line = text.read()\n",
    "    text.close()\n",
    "    word_list=line.lower().split()\n",
    "    count_dict = {}\n",
    "    for word in word_list:\n",
    "        if word[-1] in char:\n",
    "            word = word[:-1]\n",
    "        if word not in count_dict:\n",
    "            count_dict[word]=0\n",
    "    for word in word_list:\n",
    "        if word[-1] in char:\n",
    "            word = word[:-1]\n",
    "        count_dict[word]+=1\n",
    "    return count_dict\n",
    "\n",
    "name = 'twitterStream.txt'\n",
    "tf = tf(name)\n",
    "\n",
    "print 'Term Frequency:\\n\\n'\n",
    "print tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency:\n",
      "\n",
      "\n",
      "{'and': 0.84509804001425681, 'cons': 0.84509804001425681, 'https://t.co/mmmkeo3bkc': 0.84509804001425681, 'german': 0.84509804001425681, 'pros': 0.84509804001425681, 'over': 0.84509804001425681, 'https://t.co/cqewbkjnot': 0.84509804001425681, 'mission': 0.84509804001425681, 'wednesday': 0.84509804001425681, 'vote': 0.84509804001425681, 'us': 0.84509804001425681, 'in': 0.54406804435027567, 'go': 0.84509804001425681, 'says': 0.84509804001425681, 'breaking:': 0.84509804001425681, \"'islamic\": 0.84509804001425681, 'agrees': 0.36797678529459443, 'https://t.co/2xssucuxml': 0.84509804001425681, 'rt': 0.54406804435027567, 'guess': 0.84509804001425681, 'https://t.co/ura1lpqavo': 0.84509804001425681, 'sadly': 0.84509804001425681, 'i': 0.84509804001425681, 'no': 0.84509804001425681, \"it's\": 0.84509804001425681, 'support': 0.84509804001425681, '-': 0.84509804001425681, 'should': 0.84509804001425681, 'better': 0.84509804001425681, 'to': 0.54406804435027567, 'out': 0.84509804001425681, 'war': 0.84509804001425681, 'seeks': 0.84509804001425681, 'pm': 0.84509804001425681, 'ken': 0.36797678529459443, '@conflicts:': 0.84509804001425681, \"state'\": 0.84509804001425681, 'on': 0.84509804001425681, '@dwnews': 0.84509804001425681, 'approves': 0.84509804001425681, 'who': 0.36797678529459443, 'but': 0.84509804001425681, 'video:': 0.84509804001425681, 'hear': 0.84509804001425681, 'opinion': 0.84509804001425681, 'not': 0.84509804001425681, 'cabinet': 0.54406804435027567, 'with': 0.36797678529459443, 'is': 0.84509804001425681, ':s': 0.36797678529459443, 'well': 0.84509804001425681, '#syria': 0.84509804001425681, 'being': 0.84509804001425681, 'we': 0.84509804001425681, 'https://t.co/yadykuhdbz': 0.84509804001425681, '@skynews:': 0.84509804001425681, 'against': 0.84509804001425681, 'public': 0.84509804001425681, 'this': 0.84509804001425681, 'bbc': 0.84509804001425681, 'https://t.co/zv8kovbuwo': 0.84509804001425681, 'time': 0.84509804001425681, 'syria': 0.36797678529459443, 'the': 0.84509804001425681, 'asked': 0.84509804001425681}\n"
     ]
    }
   ],
   "source": [
    "#3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "\n",
    "def idf(name):\n",
    "    '''Inverse Document Frequency'''\n",
    "    docs = open(name, 'r')\n",
    "    tot_docs = len(docs.readlines())\n",
    "    count_dict = {}\n",
    "    unique = []\n",
    "    docs.close()\n",
    "    \n",
    "    #Get all unique terms\n",
    "    docs = open(name, 'r')\n",
    "    char = '.,?\"'\n",
    "    text_list = docs.read().lower().split()\n",
    "    for word in text_list:\n",
    "        if word[-1] in char:\n",
    "            word = word[:-1]\n",
    "        if word not in unique:\n",
    "            unique.append(word)\n",
    "            count_dict[word] = 0\n",
    "    docs.close()\n",
    "    \n",
    "    #Term count in each doc\n",
    "    docs = open(name, 'r')\n",
    "    for line in docs.readlines():\n",
    "        new_line = []\n",
    "        for word in line.lower().split():\n",
    "            if word[-1] in char:\n",
    "                word = word[:-1]\n",
    "            new_line.append(word)\n",
    "        for term in unique:\n",
    "            if term[-1] in char:\n",
    "                term = term[:-1]\n",
    "            if term in new_line:\n",
    "                count_dict[term] += 1\n",
    "            else:\n",
    "                pass    \n",
    "    docs.close()\n",
    "    \n",
    "    #IDF calculation\n",
    "    for key in count_dict:\n",
    "        count_dict[key] = np.log10(float(tot_docs) / float(count_dict[key]))\n",
    "    \n",
    "    return count_dict\n",
    "        \n",
    "        \n",
    "name = 'twitterStream.txt'    \n",
    "idf = idf(name)\n",
    "print 'Inverse Document Frequency:\\n\\n'\n",
    "print idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency - Inverse Document Frequency:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#syria': 0.84509804001425681,\n",
       " \"'islamic\": 0.84509804001425681,\n",
       " '-': 0.84509804001425681,\n",
       " ':s': 1.1039303558837834,\n",
       " '@conflicts:': 0.84509804001425681,\n",
       " '@dwnews': 0.84509804001425681,\n",
       " '@skynews:': 0.84509804001425681,\n",
       " 'against': 0.84509804001425681,\n",
       " 'agrees': 1.1039303558837834,\n",
       " 'and': 0.84509804001425681,\n",
       " 'approves': 0.84509804001425681,\n",
       " 'asked': 0.84509804001425681,\n",
       " 'bbc': 0.84509804001425681,\n",
       " 'being': 0.84509804001425681,\n",
       " 'better': 0.84509804001425681,\n",
       " 'breaking:': 0.84509804001425681,\n",
       " 'but': 0.84509804001425681,\n",
       " 'cabinet': 1.0881360887005513,\n",
       " 'cons': 0.84509804001425681,\n",
       " 'german': 0.84509804001425681,\n",
       " 'go': 0.84509804001425681,\n",
       " 'guess': 0.84509804001425681,\n",
       " 'hear': 0.84509804001425681,\n",
       " 'https://t.co/2xssucuxml': 0.84509804001425681,\n",
       " 'https://t.co/cqewbkjnot': 0.84509804001425681,\n",
       " 'https://t.co/mmmkeo3bkc': 0.84509804001425681,\n",
       " 'https://t.co/ura1lpqavo': 0.84509804001425681,\n",
       " 'https://t.co/yadykuhdbz': 0.84509804001425681,\n",
       " 'https://t.co/zv8kovbuwo': 0.84509804001425681,\n",
       " 'i': 0.84509804001425681,\n",
       " 'in': 1.0881360887005513,\n",
       " 'is': 0.84509804001425681,\n",
       " \"it's\": 0.84509804001425681,\n",
       " 'ken': 1.1039303558837834,\n",
       " 'mission': 0.84509804001425681,\n",
       " 'no': 0.84509804001425681,\n",
       " 'not': 0.84509804001425681,\n",
       " 'on': 0.84509804001425681,\n",
       " 'opinion': 0.84509804001425681,\n",
       " 'out': 0.84509804001425681,\n",
       " 'over': 0.84509804001425681,\n",
       " 'pm': 0.84509804001425681,\n",
       " 'pros': 0.84509804001425681,\n",
       " 'public': 0.84509804001425681,\n",
       " 'rt': 1.0881360887005513,\n",
       " 'sadly': 0.84509804001425681,\n",
       " 'says': 0.84509804001425681,\n",
       " 'seeks': 0.84509804001425681,\n",
       " 'should': 0.84509804001425681,\n",
       " \"state'\": 0.84509804001425681,\n",
       " 'support': 0.84509804001425681,\n",
       " 'syria': 1.1039303558837834,\n",
       " 'the': 0.84509804001425681,\n",
       " 'this': 0.84509804001425681,\n",
       " 'time': 0.84509804001425681,\n",
       " 'to': 1.0881360887005513,\n",
       " 'us': 0.84509804001425681,\n",
       " 'video:': 0.84509804001425681,\n",
       " 'vote': 0.84509804001425681,\n",
       " 'war': 0.84509804001425681,\n",
       " 'we': 0.84509804001425681,\n",
       " 'wednesday': 0.84509804001425681,\n",
       " 'well': 0.84509804001425681,\n",
       " 'who': 1.1039303558837834,\n",
       " 'with': 1.1039303558837834}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Multiply the term frequency for each of the term by corresponding inverse document frequency.\n",
    "\n",
    "def tfidf(tf_dict, idf_dict):\n",
    "    tfidf_dict = {}\n",
    "    for term in tf_dict.keys():\n",
    "        tfidf_dict[term] = tf_dict[term] * idf_dict[term]\n",
    "    return tfidf_dict\n",
    "\n",
    "tfidf = tfidf(tf, idf)\n",
    "print 'Term Frequency - Inverse Document Frequency:\\n\\n'\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5. Sort the terms in the descending order based on their term freq/inverse document freq scores\n",
    "\n",
    "df = pd.DataFrame(tfidf.items(),columns=['Term','TF-IDF']).sort(ascending=False,columns=['TF-IDF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ken</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>who</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>agrees</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>with</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>:s</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>syria</td>\n",
       "      <td>1.103930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rt</td>\n",
       "      <td>1.088136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>cabinet</td>\n",
       "      <td>1.088136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>to</td>\n",
       "      <td>1.088136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>1.088136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term    TF-IDF\n",
       "33      ken  1.103930\n",
       "38      who  1.103930\n",
       "16   agrees  1.103930\n",
       "46     with  1.103930\n",
       "49       :s  1.103930\n",
       "43    syria  1.103930\n",
       "18       rt  1.088136\n",
       "45  cabinet  1.088136\n",
       "28       to  1.088136\n",
       "11       in  1.088136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 terms\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
